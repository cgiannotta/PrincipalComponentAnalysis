{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import csv\n",
    "from random import choices\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "* should the bootstrapped datasets be the same size as the original dataset? or smaller?\n",
    "* do i need an instance var for the datasets or just the OOB samples for each tree?\n",
    "\n",
    "### To Do\n",
    "* find new dataset\n",
    "* function to pretty print tree\n",
    "* clean up code to make more concise (use list comprehensions instead of loops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Explain\n",
    "\n",
    "### Decision Tree\n",
    "* purity/impurity\n",
    "* entropy vs. Gini index\n",
    "* using decision tree for prediction\n",
    "\n",
    "### Random Forest\n",
    "* bootstrap aggregating\n",
    "* OOB error estimating\n",
    "* pros and cons of random forest\n",
    "\n",
    "### Extra\n",
    "* compare classification with sci-kit learn functions vs. random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and save feature and class vectors\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "#with open(\"Churn.csv\", newline=\"\") as f:\n",
    "with open(\"ChurnTestMedium.csv\", newline=\"\") as f:\n",
    "#with open(\"ChurnTest.csv\", newline=\"\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    for line in reader:\n",
    "        X.append([float(num) for num in line[0:-1]]) # save features to X list\n",
    "        y.append(int(line[-1])) # save class to y list\n",
    "\n",
    "#print(X)\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "A decision tree is a data structure used to classify new data points. You can visualize it as an upside-down tree with one main root node. This is where we begin with a full dataset. Then the data is split on a certain feature and value, resulting in two branch nodes. Each of those nodes is similarly split, and this process continues until the final split. The last split results in terminal or leaf nodes, which contain a classification for the data point in question. This is a very simple explanation of how decision trees work.\n",
    "\n",
    "The training process involves a training dataset, which is used to determine the best feature and value at which to split the data at each node. Once the tree is built (trained), we can use it to predict classifications for new data points and to evaluate the tree's accuracy or error rate. One decision tree on its own is prone to overfitting the data and is likely to have high variance. It will not perform well with new data, especially if there is no limit set on the number of times the data can be split (this is called the depth of the tree).\n",
    "\n",
    "In order to reduce the error rate and the impact of overfitting, we can train many decision trees on subsets of the same dataset. This collection of decision trees is known as a random forest. The \"random\" part of its name refers to two key elements:\n",
    "\n",
    "1. taking one dataset and randomly sampling it many times to create many datasets, none of which is exactly the same as any other\n",
    "2. using a random sample of predictors for each tree\n",
    "\n",
    "The first step, known as bootstrapping, is taking one dataset and creating new datasets from it by randomly sampling the points in the original set with replacement. For example, if we only had one dataset with points A, B, C, and D, and we wanted to create three bootstrapped datasets, we might have one with A, B, B, C, one with A, C, D, D, and one with B, C, C, D. We use the bootstrapped sets to train the decision trees in the random forest.\n",
    "\n",
    "The second random element in a random forest is the random sample of predicting variables used in each decision tree. Instead of using all predictors in all trees, we randomly select a certain number ($m$) of them. One common value for $m$ is $\\sqrt{p}$, where $p$ is the total number of predictors in the original training data.\n",
    "\n",
    "In each bootstrapped dataset, a certain number of the original datapoints will likely be left out. They are known as the out-of-bag samples, and we use them to get predictions from the trees that did not use them in training. The most common prediction for each unseen data point is the overall prediction from the random forest. This method of aggregating the predictions for each out-of-bag sample is known as bootstrapped aggregating, or bagging.\n",
    "\n",
    "Additionally, for each tree in the forest, its accuracy can be assessed using these left out data points, because they are unseen data for that tree. The average accuracy achieved by all trees in the forest in this way is known as the out-of-bag accuracy. It is useful because it reduces the variance of the random forest in comparison with an individual decision tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a predictors vector and class vector, create bootstrapped datasets\n",
    "\n",
    "class RandomForest():\n",
    "    def __init__(self, num_trees=100, depth=10): # default depth?\n",
    "        self.max_depth = depth\n",
    "        self.num_trees = num_trees\n",
    "        #self.datasets = [] # don't need instance variable for datasets, just oob samples\n",
    "        self.oob_Xs = []\n",
    "        self.oob_ys = []\n",
    "        self.forest = []\n",
    "    \n",
    "    # return one bootstrapped dataset from given data\n",
    "    def bootstrap(self, X, y):\n",
    "        sample_idxs = choices(range(len(X)), k=len(X))\n",
    "        bsX = [X[idx] for idx in sample_idxs]\n",
    "        bsY = [y[idx] for idx in sample_idxs]\n",
    "        # add data not included in boostrapped sample to oob_data list\n",
    "        oob_xs = []\n",
    "        oob_ys = []\n",
    "        for i in range(len(X)):\n",
    "            if X[i] not in bsX:\n",
    "                oob_xs.append(X[i])\n",
    "                oob_ys.append(y[i])\n",
    "        #self.oob_data.append((oob_xs, oob_ys))\n",
    "        self.oob_Xs.append(oob_xs)\n",
    "        self.oob_ys.append(oob_ys)\n",
    "        return bsX, bsY\n",
    "\n",
    "    # return list of bootstrapped datasets (1 for each tree in forest)\n",
    "    def get_datasets(self, X, y):\n",
    "        bootstrap_sets = []\n",
    "        for i in range(self.num_trees): # for each tree in the forest\n",
    "            bootstrap_sets.append(self.bootstrap(X, y)) # get boostrapped sample, add to list\n",
    "        return bootstrap_sets\n",
    "\n",
    "    # grow forest\n",
    "    def grow_forest(self, X, y):\n",
    "        datasets = self.get_datasets(X, y) # get bootstrapped datasets to use building trees\n",
    "        #self.datasets = self.get_datasets(X, y) # get bootstrapped datasets to use building trees (maybe don't need instance var for this)\n",
    "        for i in range(self.num_trees): # for each tree and each dataset\n",
    "            tree = DecisionTree(self.max_depth)\n",
    "            #tree.build_tree(X, y)\n",
    "            tree.build_tree(datasets[i][0], datasets[i][1]) # build tree with next bootstrapped dataset\n",
    "            #tree.build_tree(self.datasets[i][0], self.datasets[i][1]) # build tree with next bootstrapped dataset\n",
    "            # is this where i need to get oob predictions?\n",
    "            self.forest.append(tree) # add tree to forest\n",
    "    \n",
    "    def calc_oob_accuracy(self):\n",
    "        tot_accuracy = 0\n",
    "        for i in range(self.num_trees): # for each tree in the forest\n",
    "            preds = []\n",
    "            for x in self.oob_Xs[i]: # for each row in that tree's oob sample\n",
    "                preds.append(self.forest[i].predict(x)) # use that tree to get a prediction for that row\n",
    "            tot_accuracy += calc_accuracy(preds, self.oob_ys[i]) # keep running total of accuracy\n",
    "        return tot_accuracy / self.num_trees # return average accuracy across all trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest was grown in  0.11923340000021199  seconds.\n",
      "OOB error rate was calculated in  0.0009731000000101631  seconds.\n"
     ]
    }
   ],
   "source": [
    "# build forest\n",
    "myForest = RandomForest(num_trees=100, depth=10)\n",
    "start_forest = time.perf_counter()\n",
    "myForest.grow_forest(X, y)\n",
    "finish_forest = time.perf_counter()\n",
    "forest_time = finish_forest - start_forest\n",
    "\n",
    "# assess accuracy\n",
    "start_oob = time.perf_counter()\n",
    "myForest.calc_oob_accuracy()\n",
    "finish_oob = time.perf_counter()\n",
    "oob_time = finish_oob - start_oob\n",
    "\n",
    "# print results\n",
    "print(\"Forest was grown in \", forest_time, \" seconds.\")\n",
    "print(\"OOB error rate was calculated in \", oob_time, \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0019030416666737438"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_time / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "# load data\n",
    "# instantiate random forest class\n",
    "# call build forest method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions\n",
    "* calculate entropy\n",
    "    - entropy is a measure of how heterogeneous the data is\n",
    "    - we want to split the data in a way that will reduce the entropy of the set\n",
    "    - in a tree that classifies perfectly, each leaf would have an entropy of 0, meaning that each leaf contains only one class\n",
    "    - a tree like this would of course be overfit, so we will put a limit on the number of times the data can be split (max depth)\n",
    "    - Gini index is another metric for assessing the heterogeneity of a set of data points\n",
    "* calculate information gain\n",
    "    - information gain is the change in entropy that is achieved by splitting a dataset in a certain way\n",
    "* determine the best split\n",
    "    - this function finds the best way to split the data\n",
    "    - the best way is the way the results in the highest information gain\n",
    "    - another way of understanding this is to split in a way that reduces the entropy in the data\n",
    "    - basically we're looking to split the data so the points in each branch are as similar as possible\n",
    "    - this will result in a better classification for new datapoints\n",
    "* split data\n",
    "    - this function takes several parameters:\n",
    "        * the index of the predictor to split on\n",
    "        * the value for that predictor that should be used to split the data\n",
    "        * the predictors vector and class vector to be split\n",
    "    - for each data point, we look at the given predictor and check whether its value is less than the given value\n",
    "        * if it's less than the value, it goes into the left vector\n",
    "        * if not, it goes into the right vector\n",
    "        * the corresponding class from the class vector is similarly put into either the left or right vector\n",
    "    - then the function returns both the left and right predictor and class vectors (4 in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions to support building decision tree\n",
    "\n",
    "# calculate cost function for split (entropy)\n",
    "def calc_entropy(y_vals):\n",
    "    ent = 0\n",
    "    for y_val in set(y_vals):\n",
    "        prop = len([val for val in y_vals if val==y_val]) / len(y_vals)\n",
    "        ent += (-1 * prop) * np.log2(prop) # update entropy using formula\n",
    "    return ent\n",
    "\n",
    "# calculate information gain\n",
    "def calc_infogain(parent_yvals, left_yvals, right_yvals):\n",
    "    H = calc_entropy(parent_yvals) # entropy of parent node\n",
    "    #print(\"H: \", H)\n",
    "    H_left = calc_entropy(left_yvals) # entropy of left child node\n",
    "    #print(\"H_left: \", H_left)\n",
    "    H_right = calc_entropy(right_yvals) # entropy of right child node\n",
    "    #print(\"H_right: \", H_right)\n",
    "    P_left = len(left_yvals) / len(parent_yvals)\n",
    "    P_right = len(right_yvals) / len(parent_yvals)\n",
    "    cond_entropy = (H_left * P_left) + (H_right * P_right) # conditional entropy to compare to parent node\n",
    "    #print(\"cond_entropy: \", cond_entropy)\n",
    "    return H - cond_entropy # difference between parent node and child node entropy\n",
    "\n",
    "# determine best split (or no split)\n",
    "def best_split(X, y):\n",
    "    m = int(np.round(np.sqrt(len(X[1])),2)) # set number of predectors to test = sqrt total # predictors\n",
    "    pred_idxs_to_test = np.random.choice(range(0,len(X[1])),m, replace=False) # select random subset of predictors to test\n",
    "    pred_vals_to_test = np.mean(X, axis=0)[pred_idxs_to_test] # use mean value for each predictor as split value\n",
    "    best_idx = 0\n",
    "    best_val = 0\n",
    "    max_infogain = 0\n",
    "    #max_infogain, best_idx, best_val, best_left, best_right = 0, 9999999, 9999999, {}, {}\n",
    "    X_left, X_right, y_left, y_right = [], [], [], []\n",
    "    for i in range(len(pred_idxs_to_test)): # for each predictor in random subset\n",
    "        X_l, X_r, y_l, y_r = split_data(pred_idxs_to_test[i], pred_vals_to_test[i], X, y) # split data on mean value for each predictor\n",
    "        infogain = calc_infogain(y, y_l, y_r)\n",
    "        if infogain > max_infogain: # determine if split increases information gain / reduces entropy\n",
    "            max_infogain = infogain\n",
    "            best_idx = pred_idxs_to_test[i]\n",
    "            best_val = pred_vals_to_test[i]\n",
    "            X_left, y_left = X_l, y_l\n",
    "            X_right, y_right = X_r, y_r\n",
    "            #best_left = {\"X_left\": X_l, \"y_left\": y_l}\n",
    "            #best_right = {\"X_right\": X_r, \"y_right\": y_r}\n",
    "    #print(\"max_infogain\", max_infogain)\n",
    "    return {\"pred_idx\": best_idx, \"pred_val\": best_val, \"left\": {\"X_left\": X_left, \"y_left\": y_left}, \"right\": {\"X_right\": X_right, \"y_right\": y_right}}\n",
    "\n",
    "# split data\n",
    "def split_data(pred_idx, pred_val, X_vals, y_vals):\n",
    "        X_left, X_right, y_left, y_right = [], [], [], []\n",
    "        for i in range(len(X_vals)):\n",
    "            if X_vals[i][pred_idx] < pred_val:\n",
    "                X_left.append(X_vals[i])\n",
    "                y_left.append(y_vals[i])\n",
    "            else:\n",
    "                X_right.append(X_vals[i])\n",
    "                y_right.append(y_vals[i])\n",
    "        return X_left, X_right, y_left, y_right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decision tree summary\n",
    "* a decision tree is a structure that allows new data points to be classified based on the values of their predictors\n",
    "* it may be helpful to visualize an upside down tree, with the trunk split first into two branches/nodes (left and right)\n",
    "* each branch is then split into its own left and right branches, until an optimal depth is reached\n",
    "* for each branch, we determine the best predictor and value at which to split\n",
    "* the final left and right branches are called leaves or leaf nodes\n",
    "* the leaf nodes contain the classification for a data point\n",
    "* this structure can be followed for a new data point, travelling down and following either the left or right branch depending on the value of each predictor in the new data point\n",
    "* with a single decision tree, we would build it and then prune it back, removing nodes as needed to achieve the lowest possible error rate when predicting on new data\n",
    "* pruning minimizes the effects of overfitting, which is likely to occur with a single tree\n",
    "* when building trees as part of a random forest, however, pruning is not needed\n",
    "* it's acceptable for individual trees to be overfit, since overall the forest will have a lower prediction error rate than the individual trees\n",
    "* when the decision tree is instantiated, it takes a maximum depth, which is the number of times the original data will be split\n",
    "* the decision tree class has an instance variable, self.tree, which holds a dictionary that is built using data passed to the build_tree method once the tree is instantiated\n",
    "* the structure of the tree is as follows:\n",
    "    - each key/value pair is either a predictor index, predictor value, left branch or right branch\n",
    "    - branches are dictionaries as well\n",
    "    - each branch has key/value pairs that mimic the original structure (predictor index, predictor value, left and right branch)\n",
    "    - leaf nodes have left and right values of either 0 or 1 depending on the class assigned by the tree\n",
    "* build tree\n",
    "    - this method takes the predictors and class vectors as well as a parent node\n",
    "    - it then calls the grow tree function\n",
    "* grow tree\n",
    "    - this method takes the data and calls the best_split utility function to find the best initial split for the data\n",
    "    - the nodes returned by the best_split function become the left and right branch of the decision tree\n",
    "* split tree\n",
    "    - this method calls itself in a recursive fashion and builds the decision tree, adding another layer of depth each time it is called\n",
    "    - it takes a node, a parent node, and the current depth of the tree\n",
    "    - for each branch (left and right) of the node, it checks whether there is data in the node\n",
    "    - if further branching is needed, a new node is created with the best_split utility function, the original node's data becomes the parent node, and the function calls itself on the new node and parent node\n",
    "    - this occurs until one of three posibilities occurs:\n",
    "        * in the case of an empty node, a leaf node is created that contains the most common classification from the parent node\n",
    "        * in the case of maximum depth reached, a leaf node is created that contains the most common classification from the original data that was in that node\n",
    "        * in the case of a single data point in a node, a leaf node is created that contains that data point's class\n",
    "    - when the decision tree is built and all data split as needed, the function returns and the tree instance variable has been populated\n",
    "* predict\n",
    "    - this method takes a new data point and follows the tree to determine which class to predict\n",
    "    - it returns the class for the new data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    # create new instance of DecisionTree\n",
    "    def __init__(self, depth):\n",
    "        self.max_depth = depth\n",
    "        self.tree = {}\n",
    "    \n",
    "    # build decision tree\n",
    "    def build_tree(self, X, y, parent={}, depth=0): # does this need to take a parent node? or only split_tree needs that?\n",
    "        \n",
    "        # grow decision tree\n",
    "        def grow_tree(X, y):\n",
    "            self.tree = best_split(X, y) # get root node with best split for full data\n",
    "            parent = {} # begin with empty parent node\n",
    "            split_tree(self.tree, parent, 1) # call recursive function to build tree\n",
    "\n",
    "        # split tree, called recursively\n",
    "        def split_tree(node, parent_node, d):\n",
    "\n",
    "            # save data from node to be used in split if needed\n",
    "            left, right = node[\"left\"], node[\"right\"]\n",
    "            #print(\"left: \", left)\n",
    "            #print(\"right: \", right)\n",
    "\n",
    "            # delete data from node so can reassign best classification\n",
    "            del(node[\"left\"], node[\"right\"])\n",
    "            #print(\"node: \", node)\n",
    "\n",
    "            # check if node contains empty dataset\n",
    "            if len(left[\"X_left\"])==0 or len(right[\"X_right\"])==0:\n",
    "            #if not left[\"X_left\"] or not right[\"X_right\"]:\n",
    "                # assign each branch of the node to the most common class from the parent node\n",
    "                node[\"left\"] = max(set(parent_node[\"left\"][\"y_left\"]), key=parent_node[\"left\"][\"y_left\"].count)\n",
    "                node[\"right\"] = max(set(parent_node[\"right\"][\"y_right\"]), key=parent_node[\"right\"][\"y_right\"].count)\n",
    "                return\n",
    "\n",
    "            elif d >= self.max_depth: # check if tree has been split maximum number of times\n",
    "                # assign each branch of the node to the most common class from this node\n",
    "                node['left'] = max(set(left['y_left']), key=left['y_left'].count)\n",
    "                node['right'] = max(set(right['y_right']), key=right['y_right'].count)\n",
    "                #node[\"left\"] = max(set(parent_node[\"left\"][\"y_left\"]), key=parent_node[\"left\"][\"y_left\"].count)\n",
    "                #node[\"right\"] = max(set(parent_node[\"right\"][\"y_right\"]), key=parent_node[\"right\"][\"y_right\"].count)\n",
    "                return\n",
    "            else:\n",
    "                # check left and right datasets to see if need to split more or make terminal node\n",
    "                # assess left node\n",
    "                if len(set(left[\"y_left\"]))==1:\n",
    "                    # assign each branch of the node to the most common class from this node\n",
    "                    node['left'] = max(set(left['y_left']), key=left['y_left'].count)\n",
    "                else:\n",
    "                    # split this branch by calling split_tree function\n",
    "                    node[\"left\"] = best_split(left[\"X_left\"], left[\"y_left\"])\n",
    "                    parent = {\"left\": left, \"right\": right}\n",
    "                    split_tree(node[\"left\"], parent, d+1)\n",
    "                # assess right node\n",
    "                if len(set(right[\"y_right\"]))==1: \n",
    "                    # assign each branch of the node to the most common class from this node\n",
    "                    node['right'] = max(set(right['y_right']), key=right['y_right'].count)\n",
    "                    return\n",
    "                else:\n",
    "                    # split this branch by calling split_tree function\n",
    "                    node[\"right\"] = best_split(right[\"X_right\"], right[\"y_right\"])\n",
    "                    parent = {\"left\": left, \"right\": right}\n",
    "                    split_tree(node[\"right\"], parent, d+1)\n",
    "        \n",
    "        # call grow_tree to create decision tree\n",
    "        grow_tree(X, y)\n",
    "                \n",
    "    # predict classification for new datapoint\n",
    "    def predict(self, x):\n",
    "        curr_node = self.tree\n",
    "        while True:\n",
    "            if x[curr_node['pred_idx']] < curr_node['pred_val']:\n",
    "                if type(curr_node['left'])==int:\n",
    "                    return curr_node['left']\n",
    "                else:\n",
    "                    curr_node = curr_node['left']\n",
    "                    continue\n",
    "            else:\n",
    "                if type(curr_node['right'])==int:\n",
    "                    return curr_node['right']\n",
    "                else:\n",
    "                    curr_node = curr_node['right']\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree:  {'pred_idx': 1, 'pred_val': 0.55, 'left': {'pred_idx': 8, 'pred_val': 106449.1488888889, 'left': {'pred_idx': 3, 'pred_val': 2.75, 'left': 1, 'right': 0}, 'right': 1}, 'right': {'pred_idx': 7, 'pred_val': 0.7272727272727273, 'left': {'pred_idx': 6, 'pred_val': 0.6666666666666666, 'left': 0, 'right': 1}, 'right': 0}}\n"
     ]
    }
   ],
   "source": [
    "# test decision tree\n",
    "mytree = DecisionTree(5)\n",
    "mytree.build_tree(X, y)\n",
    "print(\"tree: \", mytree.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test predict\n",
    "preds = []\n",
    "for i in range(len(X)):\n",
    "    preds.append(mytree.predict(X[i]))\n",
    "#print(\"predictions: \", preds)\n",
    "#print(\"actual vals: \", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Accuracy\n",
    "* calc_accuracy\n",
    "    - this function takes a vector of predictions and a vector of classes\n",
    "    - it checks how many of the predictions match their corresponding classes\n",
    "    - it returns the percent of correct predictions\n",
    "    - we can call this function using the original data used to build the tree, but this will give an inflated measure of accuracy, since training data is not new data\n",
    "    - we can also use this function with the OOB samples, which will give a more accurate idea of how well the tree predicts on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should this function be with utility functions?\n",
    "# calculate accuracy\n",
    "def calc_accuracy(y_hat, y):\n",
    "    #print(\"len yhat: \", len(y_hat))\n",
    "    #print(\"len y: \", len(y))\n",
    "    correct = []\n",
    "    for i in range(len(y_hat)):\n",
    "        correct.append(y_hat[i]==y[i])\n",
    "    return sum(correct) / len(y_hat)\n",
    "\n",
    "# calculate number of accurately predicted rows\n",
    "def calc_accurate_rows(y_hat, y):\n",
    "    correct = []\n",
    "    for i in range(len(y_hat)):\n",
    "        correct.append(y_hat[i]==y[i])\n",
    "    return sum(correct)\n",
    "    \n",
    "#print(\"accuracy: \", calc_accuracy(preds, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate number of accurately predicted rows\n",
    "def calc_accurate_rows(y_hat, y):\n",
    "    correct = []\n",
    "    for i in range(len(y_hat)):\n",
    "        correct.append(y_hat[i]==y[i])\n",
    "    return sum(correct)\n",
    "\n",
    "yhat = [1, 0, 1, 0, 0, 1]\n",
    "y = [1, 0, 0, 1, 0, 1]\n",
    "calc_accurate_rows(yhat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-73316fae69ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#print(\"building tree #\", i+1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-c00d146e508a>\u001b[0m in \u001b[0;36mbuild_tree\u001b[1;34m(self, X, y, parent, depth)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;31m# call grow_tree to create decision tree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mgrow_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# predict classification for new datapoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-c00d146e508a>\u001b[0m in \u001b[0;36mgrow_tree\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# grow decision tree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mgrow_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# get root node with best split for full data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mparent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;31m# begin with empty parent node\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0msplit_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# call recursive function to build tree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-54e8433ee920>\u001b[0m in \u001b[0;36mbest_split\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mX_left\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_right\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_right\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_idxs_to_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# for each predictor in random subset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mX_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_r\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_idxs_to_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_vals_to_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# split data on mean value for each predictor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0minfogain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_infogain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_r\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minfogain\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mmax_infogain\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# determine if split increases information gain / reduces entropy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-54e8433ee920>\u001b[0m in \u001b[0;36msplit_data\u001b[1;34m(pred_idx, pred_val, X_vals, y_vals)\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[0mX_right\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0my_right\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX_left\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_right\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "accuracies = []\n",
    "for i in range(100):\n",
    "    #print(\"building tree #\", i+1)\n",
    "    tree = DecisionTree(8)\n",
    "    tree.build_tree(X, y)\n",
    "    preds = []\n",
    "    for j in range(len(X)):\n",
    "        preds.append(tree.predict(X[j]))\n",
    "    accuracies.append(calc_accuracy(preds, y))\n",
    "    #print(accuracies)\n",
    "print(\"avg accuracy: \", mean(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Steps\n",
    "In time there are changes and additions I would like to make to this code so it can do more with a greater variety of data. Here are some examples:\n",
    "\n",
    "* modify code to accept categorical (vs. only numerical) predicting variables\n",
    "\n",
    "* use similar code to build regression tree from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15381-s06/www/DTs.pdf\n",
    "\n",
    "https://towardsdatascience.com/what-is-out-of-bag-oob-score-in-random-forest-a7fa23d710\n",
    "\n",
    "https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8\n",
    "\n",
    "https://medium.com/analytics-steps/understanding-the-gini-index-and-information-gain-in-decision-trees-ab4720518ba8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
