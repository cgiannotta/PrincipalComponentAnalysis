{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from random import choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "* how to handle categorical variables?\n",
    "* make classes?\n",
    "\n",
    "### To Do\n",
    "* find new dataset\n",
    "* function to pretty print tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Explain\n",
    "\n",
    "### Decision Tree\n",
    "* purity/impurity\n",
    "* entropy vs. Gini index\n",
    "* using decision tree for prediction\n",
    "\n",
    "### Random Forest\n",
    "* bootstrap aggregating\n",
    "* OOB error estimating\n",
    "* pros and cons of random forest\n",
    "\n",
    "### Extra\n",
    "* compare classification with sci-kit learn functions vs. random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619.0, 0.0], [502.0, 0.0], [645.0, 1.0], [822.0, 1.0], [376.0, 0.0], [501.0, 1.0], [684.0, 1.0], [528.0, 1.0], [616.0, 1.0], [653.0, 1.0]]\n",
      "[1, 1, 1, 0, 1, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "#with open(\"Churn.csv\", newline=\"\") as f:\n",
    "#with open(\"ChurnTestMedium.csv\", newline=\"\") as f:\n",
    "with open(\"ChurnTest.csv\", newline=\"\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    for line in reader:\n",
    "        X.append([float(num) for num in line[0:-1]]) # save features to X list\n",
    "        y.append(int(line[-1])) # save class to y list\n",
    "        #X_list.append([float(num) for num in line[0:-1]]) # save features to X list\n",
    "        #y_list.append(int(line[-1])) # save class to y list\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[616.0, 1.0],\n",
       "  [528.0, 1.0],\n",
       "  [645.0, 1.0],\n",
       "  [645.0, 1.0],\n",
       "  [528.0, 1.0],\n",
       "  [684.0, 1.0],\n",
       "  [619.0, 0.0],\n",
       "  [645.0, 1.0],\n",
       "  [502.0, 0.0],\n",
       "  [619.0, 0.0]],\n",
       " [1, 0, 0, 1, 1, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a dataset and create bootstrapped datasets\n",
    "\n",
    "# return one bootstrapped dataset\n",
    "# take len(data) samples from the dataset with replacement\n",
    "def bootstrap(X, y):\n",
    "    bsX = choices(X, k=len(X))\n",
    "    bsY = choices(y, k=len(y))\n",
    "    return bsX, bsY\n",
    "\n",
    "#bootstrap(X, y)\n",
    "# return numtrees bootstrapped datasets (call prev function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions to support building decision tree\n",
    "\n",
    "# calculate cost function for split (entropy)\n",
    "def calc_entropy(y_vals):\n",
    "    ent = 0\n",
    "    for y_val in set(y_vals):\n",
    "        prop = len([val for val in y_vals if val==y_val]) / len(y_vals)\n",
    "        ent += (-1 * prop) * np.log2(prop) # update entropy using formula\n",
    "    return ent\n",
    "\n",
    "# split data\n",
    "def split_data(pred_idx, pred_val, X_vals, y_vals):\n",
    "        X_left, X_right, y_left, y_right = [], [], [], []\n",
    "        for i in range(len(X_vals)):\n",
    "            if X_vals[i][pred_idx] < pred_val:\n",
    "                X_left.append(X_vals[i])\n",
    "                y_left.append(y_vals[i])\n",
    "            else:\n",
    "                X_right.append(X_vals[i])\n",
    "                y_right.append(y_vals[i])\n",
    "        return X_left, X_right, y_left, y_right\n",
    "\n",
    "# calculate information gain\n",
    "def calc_infogain(parent_yvals, left_yvals, right_yvals):\n",
    "    H = calc_entropy(parent_yvals) # entropy of parent node\n",
    "    #print(\"H: \", H)\n",
    "    H_left = calc_entropy(left_yvals) # entropy of left child node\n",
    "    #print(\"H_left: \", H_left)\n",
    "    H_right = calc_entropy(right_yvals) # entropy of right child node\n",
    "    #print(\"H_right: \", H_right)\n",
    "    P_left = len(left_yvals) / len(parent_yvals)\n",
    "    P_right = len(right_yvals) / len(parent_yvals)\n",
    "    cond_entropy = (H_left * P_left) + (H_right * P_right) # conditional entropy to compare to parent node\n",
    "    #print(\"cond_entropy: \", cond_entropy)\n",
    "    return H - cond_entropy # difference between parent node and child node entropy\n",
    "\n",
    "# determine best split (or no split)\n",
    "def best_split(X, y):\n",
    "    m = int(np.round(np.sqrt(len(X[1])),2)) # set number of predectors to test = sqrt total # predictors\n",
    "    pred_idxs_to_test = np.random.choice(range(0,len(X[1])),m, replace=False) # select random subset of predictors to test\n",
    "    pred_vals_to_test = np.mean(X, axis=0)[pred_idxs_to_test] # use mean value for each predictor as split value\n",
    "    best_idx = 0\n",
    "    best_val = 0\n",
    "    max_infogain = 0\n",
    "    #max_infogain, best_idx, best_val, best_left, best_right = 0, 9999999, 9999999, {}, {}\n",
    "    X_left, X_right, y_left, y_right = [], [], [], []\n",
    "    for i in range(len(pred_idxs_to_test)): # for each predictor in random subset\n",
    "        X_l, X_r, y_l, y_r = split_data(pred_idxs_to_test[i], pred_vals_to_test[i], X, y) # split data on mean value for each predictor\n",
    "        infogain = calc_infogain(y, y_l, y_r)\n",
    "        if infogain > max_infogain: # determine if split increases information gain / reduces entropy\n",
    "            max_infogain = infogain\n",
    "            best_idx = pred_idxs_to_test[i]\n",
    "            best_val = pred_vals_to_test[i]\n",
    "            X_left, y_left = X_l, y_l\n",
    "            X_right, y_right = X_r, y_r\n",
    "            #best_left = {\"X_left\": X_l, \"y_left\": y_l}\n",
    "            #best_right = {\"X_right\": X_r, \"y_right\": y_r}\n",
    "    #print(\"max_infogain\", max_infogain)\n",
    "    return {\"pred_idx\": best_idx, \"pred_val\": best_val, \"left\": {\"X_left\": X_left, \"y_left\": y_left}, \"right\": {\"X_right\": X_right, \"y_right\": y_right}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    # create new instance of DecisionTree\n",
    "    def __init__(self, depth):\n",
    "        self.max_depth = depth\n",
    "        self.tree = {}\n",
    "    \n",
    "    # build decision tree\n",
    "    def build_tree(self, X, y, parent={}, depth=0):\n",
    "        \n",
    "        # grow decision tree\n",
    "        def grow_tree(X, y):\n",
    "            self.tree = best_split(X, y) # get root node with best split for full data\n",
    "            parent = {} # begin with empty parent node\n",
    "            split_tree(self.tree, parent, 1) # call recursive function to build tree\n",
    "\n",
    "        # split tree, called recursively\n",
    "        def split_tree(node, parent_node, d):\n",
    "\n",
    "            # save data from node to be used in split if needed\n",
    "            left, right = node[\"left\"], node[\"right\"]\n",
    "            #print(\"left: \", left)\n",
    "            #print(\"right: \", right)\n",
    "\n",
    "            # delete data from node so can reassign best classification\n",
    "            del(node[\"left\"], node[\"right\"])\n",
    "            #print(\"node: \", node)\n",
    "\n",
    "            # check if node contains empty dataset\n",
    "            if len(left[\"X_left\"])==0 or len(right[\"X_right\"])==0:\n",
    "            #if not left[\"X_left\"] or not right[\"X_right\"]:\n",
    "                # assign each branch of the node to the most common class from the parent node\n",
    "                node[\"left\"] = max(set(parent_node[\"left\"][\"y_left\"]), key=parent_node[\"left\"][\"y_left\"].count)\n",
    "                node[\"right\"] = max(set(parent_node[\"right\"][\"y_right\"]), key=parent_node[\"right\"][\"y_right\"].count)\n",
    "                return\n",
    "\n",
    "            elif d >= self.max_depth: # check if tree has been split maximum number of times\n",
    "                # assign each branch of the node to the most common class from this node\n",
    "                node['left'] = max(set(left['y_left']), key=left['y_left'].count)\n",
    "                node['right'] = max(set(right['y_right']), key=right['y_right'].count)\n",
    "                #node[\"left\"] = max(set(parent_node[\"left\"][\"y_left\"]), key=parent_node[\"left\"][\"y_left\"].count)\n",
    "                #node[\"right\"] = max(set(parent_node[\"right\"][\"y_right\"]), key=parent_node[\"right\"][\"y_right\"].count)\n",
    "                return\n",
    "            else:\n",
    "                # check left and right datasets to see if need to split more or make terminal node\n",
    "                # assess left node\n",
    "                if len(set(left[\"y_left\"]))==1:\n",
    "                    # assign each branch of the node to the most common class from this node\n",
    "                    node['left'] = max(set(left['y_left']), key=left['y_left'].count)\n",
    "                else:\n",
    "                    # split this branch by calling split_tree function\n",
    "                    node[\"left\"] = best_split(left[\"X_left\"], left[\"y_left\"])\n",
    "                    parent = {\"left\": left, \"right\": right}\n",
    "                    split_tree(node[\"left\"], parent, d+1)\n",
    "                # assess right node\n",
    "                if len(set(right[\"y_right\"]))==1: \n",
    "                    # assign each branch of the node to the most common class from this node\n",
    "                    node['right'] = max(set(right['y_right']), key=right['y_right'].count)\n",
    "                    return\n",
    "                else:\n",
    "                    # split this branch by calling split_tree function\n",
    "                    node[\"right\"] = best_split(right[\"X_right\"], right[\"y_right\"])\n",
    "                    parent = {\"left\": left, \"right\": right}\n",
    "                    split_tree(node[\"right\"], parent, d+1)\n",
    "        \n",
    "        # call grow_tree to create decision tree\n",
    "        grow_tree(X, y)\n",
    "                \n",
    "    # predict classification for new datapoint\n",
    "    def predict(self, x):\n",
    "        curr_node = self.tree\n",
    "        while True:\n",
    "            if x[curr_node['pred_idx']] < curr_node['pred_val']:\n",
    "                if type(curr_node['left'])==int:\n",
    "                    return curr_node['left']\n",
    "                else:\n",
    "                    curr_node = curr_node['left']\n",
    "                    continue\n",
    "            else:\n",
    "                if type(curr_node['right'])==int:\n",
    "                    return curr_node['right']\n",
    "                else:\n",
    "                    curr_node = curr_node['right']\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test decision tree\n",
    "mytree = DecisionTree(5)\n",
    "mytree.build_tree(X, y)\n",
    "#print(\"tree: \", mytree.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test predict\n",
    "preds = []\n",
    "for i in range(len(X)):\n",
    "    preds.append(mytree.predict(X[i]))\n",
    "#print(\"predictions: \", preds)\n",
    "#print(\"actual vals: \", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7116\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "def calc_accuracy(y_hat, y):\n",
    "    correct = []\n",
    "    for i in range(len(y_hat)):\n",
    "        correct.append(y_hat[i]==y[i])\n",
    "    return sum(correct) / len(y_hat)\n",
    "    \n",
    "print(\"accuracy: \", calc_accuracy(preds, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg accuracy:  0.770266\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "accuracies = []\n",
    "for i in range(100):\n",
    "    #print(\"building tree #\", i+1)\n",
    "    tree = DecisionTree(8)\n",
    "    tree.build_tree(X, y)\n",
    "    preds = []\n",
    "    for j in range(len(X)):\n",
    "        preds.append(tree.predict(X[j]))\n",
    "    accuracies.append(calc_accuracy(preds, y))\n",
    "    #print(accuracies)\n",
    "print(\"avg accuracy: \", mean(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate cost function for split (entropy)\n",
    "def calc_entropy(y_vals):\n",
    "    ent = 0\n",
    "    for y_val in set(y_vals):\n",
    "        prop = len([val for val in y_vals if val==y_val]) / len(y_vals)\n",
    "        ent += (-1 * prop) * np.log2(prop) # update entropy using formula\n",
    "    return ent\n",
    "\n",
    "calc_entropy(y)\n",
    "#calc_entropy(data[:,-1])\n",
    "#test_list = [1, 0, 1, 1, 1]\n",
    "#calc_entropy(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new function to split data\n",
    "def split_data(pred_idx, pred_val, X_vals, y_vals):\n",
    "    X_left, X_right, y_left, y_right = [], [], [], []\n",
    "    for i in range(len(X_vals)):\n",
    "        if X_vals[i][pred_idx] < pred_val:\n",
    "            X_left.append(X_vals[i])\n",
    "            y_left.append(y_vals[i])\n",
    "        else:\n",
    "            X_right.append(X_vals[i])\n",
    "            y_right.append(y_vals[i])\n",
    "    return X_left, X_right, y_left, y_right\n",
    "\n",
    "#X_left, X_right, y_left, y_right = split_data(0, 510, X, y)\n",
    "#print(\"left: \", X_left, y_left)\n",
    "#print(\"right: \", X_right, y_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01100085281782226"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to calculate information gain\n",
    "def calc_infogain(parent_yvals, left_yvals, right_yvals):\n",
    "    H = calc_entropy(parent_yvals) # entropy of parent node\n",
    "    #print(\"H: \", H)\n",
    "    H_left = calc_entropy(left_yvals) # entropy of left child node\n",
    "    #print(\"H_left: \", H_left)\n",
    "    H_right = calc_entropy(right_yvals) # entropy of right child node\n",
    "    #print(\"H_right: \", H_right)\n",
    "    P_left = len(left_yvals) / len(parent_yvals)\n",
    "    P_right = len(right_yvals) / len(parent_yvals)\n",
    "    cond_entropy = (H_left * P_left) + (H_right * P_right) # conditional entropy to compare to parent node\n",
    "    #print(\"cond_entropy: \", cond_entropy)\n",
    "    return H - cond_entropy # difference between parent node and child node entropy\n",
    "\n",
    "#parent_y = data[:,-1]\n",
    "#left_node, right_node = split_data(0, 600, data)\n",
    "#print(left_node)\n",
    "#print(right_node)\n",
    "#calc_infogain(parent_y, [obs[-1] for obs in left_node], [obs[-1] for obs in right_node])\n",
    "X_left, X_right, y_left, y_right = split_data(0, 600, X, y)\n",
    "calc_infogain(y, y_left, y_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = int(np.round(np.sqrt(len(X[1])),2)) # set number of predectors to test = sqrt total # predictors\n",
    "pred_idxs_to_test = np.random.choice(range(0,len(X[1])),m, replace=False) # select random subset of predictors to test\n",
    "pred_vals_to_test = np.mean(X, axis=0)[pred_idxs_to_test] # use mean value for each predictor as split value\n",
    "print(pred_vals_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new function to determine best split (or no split)\n",
    "def best_split(X, y):\n",
    "    m = int(np.round(np.sqrt(len(X[1])),2)) # set number of predectors to test = sqrt total # predictors\n",
    "    pred_idxs_to_test = np.random.choice(range(0,len(X[1])),m, replace=False) # select random subset of predictors to test\n",
    "    pred_vals_to_test = np.mean(X, axis=0)[pred_idxs_to_test] # use mean value for each predictor as split value\n",
    "    max_infogain, best_idx, best_val, best_left, best_right = 0, 9999999, 9999999, {}, {}\n",
    "    for i in range(len(pred_idxs_to_test)): # for each predictor in random subset\n",
    "        X_left, X_right, y_left, y_right = split_data(pred_idxs_to_test[i], pred_vals_to_test[i], X, y) # split data on mean value for each predictor\n",
    "        infogain = calc_infogain(y, y_left, y_right)\n",
    "        if infogain > max_infogain: # determine if split increases information gain / reduces entropy\n",
    "            max_infogain = infogain\n",
    "            best_idx = pred_idxs_to_test[i]\n",
    "            best_val = pred_vals_to_test[i]\n",
    "            best_left = {\"X_left\": X_left, \"y_left\": y_left}\n",
    "            best_right = {\"X_right\": X_right, \"y_right\": y_right}\n",
    "    #print(\"max_infogain\", max_infogain)\n",
    "    node = {\"pred_idx\": best_idx, \"pred_val\": best_val, \"left\": best_left, \"right\": best_right}\n",
    "    print(\"node from best split: \", node)\n",
    "    return node\n",
    "    #return {\"pred_idx\": best_idx, \"pred_val\": best_val, \"left\": best_left, \"right\": best_right}\n",
    "\n",
    "#best_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to determine best split (or no split)\n",
    "'''def best_split(data):\n",
    "    y_vals = data[:,-1] # extract response values from data\n",
    "    #max_infogain = 0\n",
    "    m = int(np.round(np.sqrt(data.shape[1]-1))) # set number of predectors to test = sqrt total # predictors\n",
    "    pred_idxs_to_test = np.random.choice(range(0,data.shape[1]-1),m, replace=False) # select random subset of predictors to test\n",
    "    pred_vals_to_test = np.mean(data[:,0:-1], axis=0)[pred_idxs_to_test] # use mean value for each predictor as split value\n",
    "    #print(pred_idxs_to_test)\n",
    "    #max_infogain, best_idx, best_val, best_left, best_right = 0, 999, 999, list(), list()\n",
    "    max_infogain, best_idx, best_val, best_left, best_right = 0, 999, 999, None, None\n",
    "    for i in range(len(pred_idxs_to_test)): # for each predictor in random subset\n",
    "        left, right = split_data(pred_idxs_to_test[i], pred_vals_to_test[i], data) # split data on mean value for each predictor\n",
    "        #infogain = calc_infogain(y_vals, [obs[-1] for obs in left], [obs[-1] for obs in right])\n",
    "        infogain = calc_infogain(y_vals, left[:,-1], right[:,-1])\n",
    "        if infogain > max_infogain: # determine if split increases information gain / reduces entropy\n",
    "            max_infogain = infogain\n",
    "            best_idx = pred_idxs_to_test[i]\n",
    "            best_val = pred_vals_to_test[i]\n",
    "            best_left, best_right = left, right\n",
    "    print(\"max_infogain\", max_infogain)\n",
    "    return {\"pred_idx\": best_idx, \"pred_val\": best_val, \"left\": best_left, \"right\": best_right}\n",
    "\n",
    "best_split(data)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old\n",
    "# function to determine best split (or no split)\n",
    "'''def best_split(data):\n",
    "    y_vals = data[:,-1] # extract response values from data\n",
    "    max_infogain = 0\n",
    "    m = int(np.round(np.sqrt(data.shape[1]-1))) # set number of predectors to test = sqrt total predictors\n",
    "    pred_vals_to_test = np.random.choice(range(0,data.shape[1]-1),m, replace=False) # select random subset of predictors to test\n",
    "    print(pred_vals_to_test)\n",
    "    #max_infogain, best_idx, best_val, best_left, best_right = 0, 999, 999, list(), list()\n",
    "    for idx in pred_vals_to_test:\n",
    "        for row in data:\n",
    "            left, right = split_data(idx, row[idx], data)\n",
    "            infogain = calc_infogain(y_vals, [obs[-1] for obs in left], [obs[-1] for obs in right])\n",
    "            #print(\"infogain: \", infogain)\n",
    "            if infogain > max_infogain:\n",
    "                max_infogain = infogain\n",
    "                best_idx = idx\n",
    "                best_val = row[idx]\n",
    "                best_left, best_right = left, right\n",
    "    print(\"max_infogain\", max_infogain)\n",
    "    return {\"pred_idx\": best_idx, \"pred_val\": best_val, \"left\": best_left, \"right\": best_right}\n",
    "\n",
    "best_split(data)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build decision tree\n",
    "def build_tree(X, y):\n",
    "    #tree = best_split(X, y) # get root node with best split for full data\n",
    "    depth = 0 # set initial depth to 0\n",
    "    parent = {} # begin with empty parent node\n",
    "    split_tree(tree, parent, depth) # call recursive function to build tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to split tree, called recursively\n",
    "def split_tree(node, parent_node, d):\n",
    "    \n",
    "    # save data from node to be used in split if needed\n",
    "    left, right = node[\"left\"], node[\"right\"]\n",
    "    \n",
    "    # delete data from node so can reassign best classification\n",
    "    del(node[\"left\"], node[\"right\"])\n",
    "    \n",
    "    # check if node contains empty dataset\n",
    "    if len(left[\"X_left\"])==0 or len(right[\"X_right\"])==0:\n",
    "        # assign each branch of the node to the most common class from the parent node\n",
    "        node[\"left\"] = max(set(parent_node[\"left\"][\"y_left\"]), key=parent_node[\"left\"][\"y_left\"].count)\n",
    "        node[\"right\"] = max(set(parent_node[\"right\"][\"y_right\"]), key=parent_node[\"right\"][\"y_right\"].count)\n",
    "        return\n",
    "    \n",
    "    # check if tree has been split maximum number of times\n",
    "    elif d >= max_depth:\n",
    "        # assign each branch of the node to the most common class from the parent node\n",
    "        node[\"left\"] = max(set(parent_node[\"left\"][\"y_left\"]), key=parent_node[\"left\"][\"y_left\"].count)\n",
    "        node[\"right\"] = max(set(parent_node[\"right\"][\"y_right\"]), key=parent_node[\"right\"][\"y_right\"].count)\n",
    "        return\n",
    "    else:\n",
    "        # check left and right datasets to see if need to split more or make terminal node\n",
    "        # assess left node\n",
    "        if len(set(left[\"y_left\"]))==1:\n",
    "            # assign left branch of the node to the most common class from the parent node\n",
    "            node[\"left\"] = max(set(parent_node[\"left\"][\"y_left\"]), key=parent_node[\"left\"][\"y_left\"].count)\n",
    "        else:\n",
    "            # split this branch by calling split_tree function\n",
    "            node[\"left\"] = best_split(left[\"X_left\"], left[\"y_left\"])\n",
    "            parent = {\"left\": left, \"right\": right}\n",
    "            split_tree(node[\"left\"], parent, d+1)\n",
    "        # assess right node\n",
    "        if len(set(right[\"y_right\"]))==1:\n",
    "            # assign right branch of the node to the most common class from the parent node\n",
    "            node[\"right\"] = max(set(parent_node[\"right\"][\"y_right\"]), key=parent_node[\"right\"][\"y_right\"].count)\n",
    "            return\n",
    "        else:\n",
    "            # split this branch by calling split_tree function\n",
    "            node[\"right\"] = best_split(right[\"X_right\"], right[\"y_right\"])\n",
    "            parent = {\"left\": left, \"right\": right}\n",
    "            split_tree(node[\"right\"], parent, d+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of trees\n",
    "num_trees = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree:  {'pred_idx': 7, 'pred_val': 0.55, 'left': {'pred_idx': 8, 'pred_val': 104090.83333333333, 'left': {'pred_idx': 5, 'pred_val': 1.3333333333333333, 'left': 1, 'right': 1}, 'right': 0}, 'right': {'pred_idx': 3, 'pred_val': 3.909090909090909, 'left': {'pred_idx': 1, 'pred_val': 0.6666666666666666, 'left': {'pred_idx': 8, 'pred_val': 100768.155, 'left': 0, 'right': 0}, 'right': 0}, 'right': 0}}\n"
     ]
    }
   ],
   "source": [
    "# main function\n",
    "# set maximum depth for tree\n",
    "max_depth = 5\n",
    "# get root node for tree\n",
    "tree = best_split(X, y)\n",
    "# split root node recursively\n",
    "build_tree(X, y)\n",
    "print(\"tree: \", tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict classification for new datapoint\n",
    "def predict(x):\n",
    "    curr_node = tree\n",
    "    while True:\n",
    "        if x[curr_node['pred_idx']] < curr_node['pred_val']:\n",
    "            if type(curr_node['left'])==int:\n",
    "                return curr_node['left']\n",
    "            else:\n",
    "                curr_node = curr_node['left']\n",
    "                continue\n",
    "        else:\n",
    "            if type(curr_node['right'])==int:\n",
    "                return curr_node['right']\n",
    "            else:\n",
    "                curr_node = curr_node['right']\n",
    "                continue\n",
    "\n",
    "#preds = []\n",
    "#for i in range(len(X)):\n",
    "#    preds.append(predict(X[i]))\n",
    "#print(\"predictions: \", preds)\n",
    "#print(\"actual vals: \", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate accuracy\n",
    "def calc_accuracy(y_hat, y):\n",
    "    correct = []\n",
    "    for i in range(len(y_hat)):\n",
    "        correct.append(y_hat[i]==y[i])\n",
    "    return sum(correct) / len(y_hat)\n",
    "    \n",
    "#print(\"accuracy: \", calc_accuracy(preds, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg accuracy:  0.694758\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "accuracies = []\n",
    "for i in range(100):\n",
    "    #print(\"building tree #\", i+1)\n",
    "    # set maximum depth for tree\n",
    "    max_depth = 5\n",
    "    # get root node for tree\n",
    "    tree = best_split(X, y)\n",
    "    # split root node recursively\n",
    "    build_tree(X, y)\n",
    "    preds = []\n",
    "    for j in range(len(X)):\n",
    "        preds.append(predict(X[j]))\n",
    "    accuracies.append(calc_accuracy(preds, y))\n",
    "print(\"avg accuracy: \", mean(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15381-s06/www/DTs.pdf\n",
    "https://towardsdatascience.com/what-is-out-of-bag-oob-score-in-random-forest-a7fa23d710"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
