{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import csv\n",
    "from random import choices, choice\n",
    "import time\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "* should the bootstrapped datasets be the same size as the original dataset? or smaller?\n",
    "* do i need an instance var for the datasets or just the OOB samples for each tree?\n",
    "\n",
    "### To Do\n",
    "* use consistent vocabulary (predictor vector vs list vs. features, etc...)\n",
    "* function to pretty print tree\n",
    "* experiment to improve accuracy (# predictors?)\n",
    "* clean up code to make more concise (use list comprehensions instead of loops)\n",
    "* compare this RF with sci-kit learn functions\n",
    "* delete old code, unnecessary print statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"forest.jpg\">\n",
    "\n",
    "#### Overview\n",
    "In this article, I will explain how to build a random forest classifier from scratch using only basic Python code (no scikit-learn functions or other black box methods). \n",
    "\n",
    "To understand random forests, you first need to known how a decision tree works. A decision tree is a data structure used to classify new data points. You can visualize it as an upside-down tree with one main root node. This is where we begin with a training dataset. The data is split on a certain feature and value, resulting in two branch nodes. Each of those nodes is similarly split, and this process continues until the final split. The last split results in terminal or leaf nodes, which contain a classification for the data point in question. This is a very simple explanation of how decision trees work.\n",
    "\n",
    "When building the tree, we use a training dataset to determine the best feature and value at which to split the data at each node. Once the tree is built (trained), we can use it to predict classifications for new data points and to evaluate the tree's accuracy or error rate. One decision tree on its own is prone to overfitting the data and is likely to have high variance. It will not perform well with new data, especially if there is no limit set on the number of times the data can be split (this is called the depth of the tree).\n",
    "\n",
    "In order to improve accuracy and reduce the impact of overfitting, we can train many decision trees on subsets of the same dataset. This collection of decision trees is known as a random forest. The \"random\" part of its name refers to two key elements:\n",
    "\n",
    "1. taking one dataset and randomly sampling it many times to create many datasets, none of which is exactly the same as any other\n",
    "2. using a random sample of predictors for each tree\n",
    "\n",
    "In practice, random forest classifiers are a highly accurate method for classification problems. [Click here](https://statquest.org/statquest-decision-trees/) for an excellent introduction to decision trees and [here](https://statquest.org/statquest-random-forests-part-1-building-using-and-evaluating/) for more information about random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data\n",
    "\n",
    "We begin by reading a csv file and saving it in two lists. For each row in the dataset, we save the predicting variables to one list and the response variable to another list. What we end up with is a list of lists, X, which contains predicting variables for all data points, and another list, y, which contains the classifications for each datapoint in the X list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five rows of predicting variables:\n",
      "[62849.0, 80079.0, 82.42, 6.0, 1.0, 0.0, 0.0, 15400.0, 15400.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[55159.0, 65808.0, 84.94, 4.0, 3.0, 0.0, 24255.0, 93303.0, 93303.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2990.0, 0.0, 1.0, 0.0, 0.0]\n",
      "[52303.0, 90182.0, 59.88, 2.0, 2.0, 0.0, 3315.0, 10000.0, 3315.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0]\n",
      "[55948.0, 66982.0, 85.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[41694.0, 79000.0, 54.81, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "First five classes: [0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# load data and save feature and class vectors\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "#with open(\"Churn.csv\", newline=\"\") as f:\n",
    "#with open(\"ChurnTestMedium.csv\", newline=\"\") as f:\n",
    "#with open(\"Breast_cancer_data.csv\", newline=\"\") as f:\n",
    "with open(\"loanSample.csv\", newline=\"\") as f:\n",
    "#with open(\"ChurnTest.csv\", newline=\"\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    for line in reader:\n",
    "        X.append([float(num) for num in line[0:-1]]) # save predictors to X list\n",
    "        y.append(int(line[-1])) # save class to y list\n",
    "\n",
    "# Break data into training and testing sets\n",
    "X_train = X[:4000]\n",
    "X_test = X[4000:5000]\n",
    "y_train = y[:4000]\n",
    "y_test = y[4000:5000]\n",
    "\n",
    "print(\"First five rows of predicting variables (training set):\")\n",
    "for i in range(5):\n",
    "    print(X_test[i])\n",
    "print()\n",
    "print(\"First five classes (training set):\", y_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions\n",
    "With the data loaded, we can begin creating the DecisionTree class. First, there are a number of utility functions that support this class. These functions help with splitting the data, which can be broken down into several steps.\n",
    "\n",
    "#### Entropy\n",
    "First, given a list of response variables/classes, we need to determine the entropy, which is our measure of how heterogenous or pure each node in the tree is. We want to split the data in a way that will reduce the entropy of the set. In a tree that classifies perfectly, each leaf would have an entropy of 0, meaning that each leaf contains only one class. A tree like this would be overfit, so we will put a limit on the number of times the data can be split (this limit is an instance variable for the tree, called max_depth). Gini index is another metric for assessing the heterogeneity of a set of datapoints. [Click here](https://towardsdatascience.com/gini-index-vs-information-entropy-7a7e4fed3fcb) for a detailed explanation of both entropy and Gini metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cost function for split (entropy)\n",
    "def calc_entropy(y_vals):\n",
    "    ent = 0\n",
    "    for y_val in set(y_vals):\n",
    "        prop = len([val for val in y_vals if val==y_val]) / len(y_vals)\n",
    "        ent += (-1 * prop) * np.log2(prop) # update entropy using formula\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information Gain\n",
    "Next, given a node and a potential split for its data, we need to determine whether we should split the node or leave it as a terminal/leaf node. To do this we need to know the affect on entropy of the split: will entropy be reduced by further splitting the data in the node, and by how much? Thus we have a function that calculates the information gain, or change in entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate information gain\n",
    "def calc_infogain(parent_yvals, left_yvals, right_yvals):\n",
    "    H = calc_entropy(parent_yvals) # entropy of parent node\n",
    "    #print(\"H: \", H)\n",
    "    H_left = calc_entropy(left_yvals) # entropy of left child node\n",
    "    #print(\"H_left: \", H_left)\n",
    "    H_right = calc_entropy(right_yvals) # entropy of right child node\n",
    "    #print(\"H_right: \", H_right)\n",
    "    P_left = len(left_yvals) / len(parent_yvals)\n",
    "    P_right = len(right_yvals) / len(parent_yvals)\n",
    "    cond_entropy = (H_left * P_left) + (H_right * P_right) # conditional entropy to compare to parent node\n",
    "    #print(\"cond_entropy: \", cond_entropy)\n",
    "    return H - cond_entropy # difference between parent node and child node entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the Data\n",
    "\n",
    "We also need a function that uses the previous functions to find the best way to split the data. The best way is the way that reults in the highest information gain. In other words, the split that reduces entropy the most and results in nodes containing classes that are as similar as possible. This will result in a better classification for new datapoints. To do this, we select a random subset of predictor variables and iterate over them. For each predictor, we find the information gain if we split on the mean value for that predictor. Once we find the maximum information gain, we know that's the best way to split the data.\n",
    "\n",
    "Putting all these functions together, we can take a dataset, determine the best way to split it, and split it into two new nodes. The split function takes several parameters:\n",
    "\n",
    "* the index of the predictor to split on\n",
    "* the value for that predictor that should be used to split the data\n",
    "* the predictors vector and class vector to be split\n",
    "\n",
    "Then, for each data point, we look at the given predictor and check whether its value is less than the given value:\n",
    "\n",
    "* If it's less than the value, it goes into the left vector\n",
    "* If not, it goes into the right vector\n",
    "* The corresponding class from the class vector is similarly put into either the left or right vector\n",
    "\n",
    "The function returns both the left and right predictor and class vectors (4 in total) in the form of a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine best split (or no split)\n",
    "def best_split(X, y):\n",
    "    m = int(np.round(np.sqrt(len(X[1])),2)) # set number of predectors to test = sqrt total # predictors\n",
    "    pred_idxs_to_test = np.random.choice(range(0,len(X[1])),m, replace=False) # select random subset of predictors to test\n",
    "    pred_vals_to_test = np.mean(X, axis=0)[pred_idxs_to_test] # use mean value for each predictor as split value\n",
    "    best_idx = 0\n",
    "    best_val = 0\n",
    "    max_infogain = 0\n",
    "    #max_infogain, best_idx, best_val, best_left, best_right = 0, 9999999, 9999999, {}, {}\n",
    "    X_left, X_right, y_left, y_right = [], [], [], []\n",
    "    for i in range(len(pred_idxs_to_test)): # for each predictor in random subset\n",
    "        X_l, X_r, y_l, y_r = split_data(pred_idxs_to_test[i], pred_vals_to_test[i], X, y) # split data on mean value for each predictor\n",
    "        infogain = calc_infogain(y, y_l, y_r)\n",
    "        if infogain > max_infogain: # determine if split increases information gain / reduces entropy\n",
    "            max_infogain = infogain\n",
    "            best_idx = pred_idxs_to_test[i]\n",
    "            best_val = pred_vals_to_test[i]\n",
    "            X_left, y_left = X_l, y_l\n",
    "            X_right, y_right = X_r, y_r\n",
    "            #best_left = {\"X_left\": X_l, \"y_left\": y_l}\n",
    "            #best_right = {\"X_right\": X_r, \"y_right\": y_r}\n",
    "    #print(\"max_infogain\", max_infogain)\n",
    "    return {\"pred_idx\": best_idx, \"pred_val\": best_val, \"left\": {\"X_left\": X_left, \"y_left\": y_left}, \"right\": {\"X_right\": X_right, \"y_right\": y_right}}\n",
    "\n",
    "# split data\n",
    "def split_data(pred_idx, pred_val, X_vals, y_vals):\n",
    "        X_left, X_right, y_left, y_right = [], [], [], []\n",
    "        for i in range(len(X_vals)):\n",
    "            if X_vals[i][pred_idx] < pred_val:\n",
    "                X_left.append(X_vals[i])\n",
    "                y_left.append(y_vals[i])\n",
    "            else:\n",
    "                X_right.append(X_vals[i])\n",
    "                y_right.append(y_vals[i])\n",
    "        return X_left, X_right, y_left, y_right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Predictions\n",
    "\n",
    "The last utility function takes a list of predictions and a list of labels and simply calculates the accuracy rate for the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy of predictions\n",
    "def calc_accuracy(y_hat, y):\n",
    "    correct = []\n",
    "    for i in range(len(y_hat)):\n",
    "        correct.append(y_hat[i]==y[i])\n",
    "    return sum(correct) / len(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tree.jpg\">\n",
    "\n",
    "### Decision Tree Summary\n",
    "\n",
    "For each branch of the decision tree, we look at the datapoints in that branch and determine the best predictor and predictor value at which to split the data. We split the data into two nodes using that predictor and value. Then we take the resulting nodes and split each of them in the same way. The final left and right branches are called terminal or leaf nodes. The leaf nodes contain the classification for a data point. Once the tree is trained, its structure can be followed for a new data point, travelling down and following either the left or right branch depending on the value of each predictor in the new data point.\n",
    "\n",
    "With a single decision tree, we would build it and then prune it back, removing nodes as needed to achieve the highest possible accuracy when predicting on new data. Pruning minimizes the effects of overfitting, which is likely to occur with a single tree. When building trees as part of a random forest, however, pruning is not needed. It's acceptable for individual trees to be overfit, since overall the forest will have a lower prediction error rate than the individual trees.\n",
    "\n",
    "When the decision tree is instantiated, it takes a maximum depth, which is the number of times the original data will be split. The decision tree class has an instance variable, self.tree, which holds a dictionary that is populated using data passed to the build_tree method once the tree is instantiated.\n",
    "\n",
    "The structure of the tree is as follows:\n",
    "\n",
    "* Each key/value pair is either a predictor index, predictor value, left branch or right branch\n",
    "* Branches are dictionaries as well\n",
    "* Each branch has key/value pairs that mimic the original structure (predictor index, predictor value, left and right branch)\n",
    "* Leaf nodes have left and right values of either 0 or 1 depending on the class assigned by the tree\n",
    "\n",
    "Here are the key methods of the DecisionTree class:\n",
    "* build_tree:\n",
    "    - This method takes the predictors and class vectors as well as a parent node, and then it calls the grow_tree function.\n",
    "* grow_tree\n",
    "    - This method takes the data and calls the best_split utility function to find the best initial split for the data.\n",
    "    - The nodes returned by the best_split function become the left and right branch of the decision tree.\n",
    "* split_tree\n",
    "    - This method calls itself in a recursive fashion and builds the decision tree, adding another layer of depth each time it is called.\n",
    "    - It takes a node, a parent node, and the current depth of the tree.\n",
    "    - For each branch (left and right) of the node, it checks whether there is data in the node.\n",
    "    - If further branching is needed, a new node is created with the best_split utility function, the original node's data becomes the parent node, and the function calls itself on the new node and parent node.\n",
    "    - This occurs until one of three posibilities takes place:\n",
    "        * In the case of an empty node, a leaf node is created that contains the most common classification from the parent node.\n",
    "        * In the case of maximum depth reached, a leaf node is created that contains the most common classification from the original data that was in that node.\n",
    "        * In the case of a single data point in a node, a leaf node is created that contains that data point's class.\n",
    "    - When the decision tree is built and all data split as needed, the function returns and the tree instance variable has been populated\n",
    "* predict\n",
    "    - This method takes a new data point and follows the structure of the tree to determine which class to predict.\n",
    "    - It returns the class for the new data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    # create new instance of DecisionTree\n",
    "    def __init__(self, depth):\n",
    "        self.max_depth = depth\n",
    "        self.tree = {}\n",
    "    \n",
    "    # build decision tree\n",
    "    def build_tree(self, X, y, parent={}, depth=0): # does this need to take a parent node? or only split_tree needs that?\n",
    "        \n",
    "        # grow decision tree\n",
    "        def grow_tree(X, y):\n",
    "            self.tree = best_split(X, y) # get root node with best split for full data\n",
    "            parent = {} # begin with empty parent node\n",
    "            split_tree(self.tree, parent, 1) # call recursive function to build tree\n",
    "\n",
    "        # split tree, called recursively\n",
    "        def split_tree(node, parent_node, d):\n",
    "\n",
    "            # save data from node to be used in split if needed\n",
    "            left, right = node[\"left\"], node[\"right\"]\n",
    "            #print(\"left: \", left)\n",
    "            #print(\"right: \", right)\n",
    "\n",
    "            # delete data from node so can reassign best classification\n",
    "            del(node[\"left\"], node[\"right\"])\n",
    "            #print(\"node: \", node)\n",
    "\n",
    "            # check if node contains empty dataset\n",
    "            if len(left[\"X_left\"])==0 or len(right[\"X_right\"])==0:\n",
    "            #if not left[\"X_left\"] or not right[\"X_right\"]:\n",
    "                # assign each branch of the node to the most common class from the parent node\n",
    "                node[\"left\"] = max(set(parent_node[\"left\"][\"y_left\"]), key=parent_node[\"left\"][\"y_left\"].count)\n",
    "                node[\"right\"] = max(set(parent_node[\"right\"][\"y_right\"]), key=parent_node[\"right\"][\"y_right\"].count)\n",
    "                return\n",
    "\n",
    "            elif d >= self.max_depth: # check if tree has been split maximum number of times\n",
    "                # assign each branch of the node to the most common class from this node\n",
    "                node['left'] = max(set(left['y_left']), key=left['y_left'].count)\n",
    "                node['right'] = max(set(right['y_right']), key=right['y_right'].count)\n",
    "                #node[\"left\"] = max(set(parent_node[\"left\"][\"y_left\"]), key=parent_node[\"left\"][\"y_left\"].count)\n",
    "                #node[\"right\"] = max(set(parent_node[\"right\"][\"y_right\"]), key=parent_node[\"right\"][\"y_right\"].count)\n",
    "                return\n",
    "            else:\n",
    "                # check left and right datasets to see if need to split more or make terminal node\n",
    "                # assess left node\n",
    "                if len(set(left[\"y_left\"]))==1:\n",
    "                    # assign each branch of the node to the most common class from this node\n",
    "                    node['left'] = max(set(left['y_left']), key=left['y_left'].count)\n",
    "                else:\n",
    "                    # split this branch by calling split_tree function\n",
    "                    node[\"left\"] = best_split(left[\"X_left\"], left[\"y_left\"])\n",
    "                    parent = {\"left\": left, \"right\": right}\n",
    "                    split_tree(node[\"left\"], parent, d+1)\n",
    "                # assess right node\n",
    "                if len(set(right[\"y_right\"]))==1: \n",
    "                    # assign each branch of the node to the most common class from this node\n",
    "                    node['right'] = max(set(right['y_right']), key=right['y_right'].count)\n",
    "                    return\n",
    "                else:\n",
    "                    # split this branch by calling split_tree function\n",
    "                    node[\"right\"] = best_split(right[\"X_right\"], right[\"y_right\"])\n",
    "                    parent = {\"left\": left, \"right\": right}\n",
    "                    split_tree(node[\"right\"], parent, d+1)\n",
    "        \n",
    "        # call grow_tree to create decision tree\n",
    "        grow_tree(X, y)\n",
    "                \n",
    "    # predict classification for new datapoint\n",
    "    def predict(self, x):\n",
    "        curr_node = self.tree # start at the root node\n",
    "        while True:\n",
    "            if x[curr_node['pred_idx']] < curr_node['pred_val']: # determine which branch to follow\n",
    "                if type(curr_node['left'])==int: # if a terminal node is reached\n",
    "                    return curr_node['left'] # return that classification\n",
    "                else:\n",
    "                    curr_node = curr_node['left'] # if a terminal node is not reached, continue down the tree\n",
    "                    continue\n",
    "            else:\n",
    "                if type(curr_node['right'])==int: # if a terminal node is reached\n",
    "                    return curr_node['right'] # return that classification\n",
    "                else:\n",
    "                    curr_node = curr_node['right']  # if a terminal node is not reached, continue down the tree\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree:  {'pred_idx': 4, 'pred_val': 1.0685, 'left': {'pred_idx': 0, 'pred_val': 54227.535960591136, 'left': {'pred_idx': 17, 'pred_val': 0.09611712285168682, 'left': {'pred_idx': 10, 'pred_val': 0.010541110330288124, 'left': {'pred_idx': 4, 'pred_val': 0.17530163236337828, 'left': 0, 'right': 0}, 'right': {'pred_idx': 18, 'pred_val': 0.2857142857142857, 'left': 0, 'right': 0}}, 'right': {'pred_idx': 14, 'pred_val': 14222.635135135135, 'left': {'pred_idx': 5, 'pred_val': 0.014388489208633094, 'left': 0, 'right': 1}, 'right': {'pred_idx': 1, 'pred_val': 65394.88888888889, 'left': 0, 'right': 0}}}, 'right': {'pred_idx': 16, 'pred_val': 43.217096336499324, 'left': {'pred_idx': 10, 'pred_val': 0.006151742993848257, 'left': {'pred_idx': 5, 'pred_val': 0.05357142857142857, 'left': 0, 'right': 0}, 'right': {'pred_idx': 19, 'pred_val': 0.14285714285714285, 'left': 0, 'right': 1}}, 'right': {'pred_idx': 19, 'pred_val': 0.36363636363636365, 'left': {'pred_idx': 11, 'pred_val': 0.25, 'left': 0, 'right': 0}, 'right': {'pred_idx': 13, 'pred_val': 103406.0, 'left': 1, 'right': 0}}}}, 'right': {'pred_idx': 9, 'pred_val': 0.1099476439790576, 'left': {'pred_idx': 18, 'pred_val': 0.29106945975744214, 'left': {'pred_idx': 4, 'pred_val': 3.5216763005780347, 'left': {'pred_idx': 8, 'pred_val': 340293.9496717724, 'left': 0, 'right': 0}, 'right': {'pred_idx': 8, 'pred_val': 1116551.655319149, 'left': 0, 'right': 0}}, 'right': {'pred_idx': 5, 'pred_val': 1.0465116279069768, 'left': {'pred_idx': 4, 'pred_val': 4.031645569620253, 'left': 0, 'right': 0}, 'right': {'pred_idx': 6, 'pred_val': 1047642.8070175438, 'left': 0, 'right': 0}}}, 'right': {'pred_idx': 19, 'pred_val': 0.3541666666666667, 'left': {'pred_idx': 3, 'pred_val': 8.297297297297296, 'left': {'pred_idx': 14, 'pred_val': 159865.69565217392, 'left': 0, 'right': 0}, 'right': 0}, 'right': {'pred_idx': 7, 'pred_val': 4676649.454545454, 'left': {'pred_idx': 3, 'pred_val': 8.8, 'left': 0, 'right': 0}, 'right': 1}}}}\n"
     ]
    }
   ],
   "source": [
    "# test decision tree\n",
    "mytree = DecisionTree(5)\n",
    "mytree.build_tree(X_train, y_train)\n",
    "print(\"tree: \", mytree.tree)\n",
    "\n",
    "# test predict\n",
    "preds = []\n",
    "for i in range(len(X_train)):\n",
    "    preds.append(mytree.predict(X_train[i]))\n",
    "#print(\"predictions: \", preds)\n",
    "#print(\"actual vals: \", y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Summary\n",
    "\n",
    "In the RandomForest class, a given number of decision trees are built using bootstrapped datasets (explained below). The accuracy rates of all trees are aggregated to determine the overall accuracy of the forest.\n",
    "\n",
    "The first step, known as bootstrapping, is taking one dataset and creating new datasets from it by randomly sampling the points in the original set with replacement. For example, if we only had one dataset with points A, B, C, and D, and we wanted to create three bootstrapped datasets, we might have one with A, B, B, C, one with A, C, D, D, and one with B, C, C, D. We use the bootstrapped sets to train the decision trees in the random forest.\n",
    "\n",
    "The second random element in a random forest is the random sample of predicting variables used in each decision tree. Instead of using all predictors in all trees, we randomly select a certain number ($m$) of them. One common value for $m$ is $\\sqrt{p}$, where $p$ is the total number of predictors in the original training data.\n",
    "\n",
    "In each bootstrapped dataset, a certain number of the original datapoints will likely be left out. They are known as the out-of-bag samples, and we use them to get predictions from the trees that did not use them in training. The most common prediction for each unseen data point is the overall prediction from the random forest. This method of aggregating the predictions for each out-of-bag sample is known as bootstrapped aggregating, or bagging.\n",
    "\n",
    "Additionally, for each tree in the forest, its accuracy can be assessed using these left out data points, because they are unseen data for that tree. The average accuracy achieved by all trees in the forest in this way is known as the out-of-bag accuracy. It is useful because it reduces the variance of the random forest in comparison with an individual decision tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a predictors vector and class vector, create bootstrapped datasets\n",
    "\n",
    "class RandomForest():\n",
    "    def __init__(self, num_trees=100, depth=10): # default depth?\n",
    "        self.max_depth = depth\n",
    "        self.num_trees = num_trees\n",
    "        #self.datasets = [] # don't need instance variable for datasets, just oob samples\n",
    "        self.oob_Xs = []\n",
    "        self.oob_ys = []\n",
    "        self.forest = []\n",
    "    \n",
    "    # return one bootstrapped dataset from given data\n",
    "    def bootstrap(self, X, y):\n",
    "        sample_idxs = choices(range(len(X)), k=len(X))\n",
    "        bsX = [X[idx] for idx in sample_idxs]\n",
    "        bsY = [y[idx] for idx in sample_idxs]\n",
    "        # add data not included in boostrapped sample to oob_data list\n",
    "        oob_xs = []\n",
    "        oob_ys = []\n",
    "        for i in range(len(X)):\n",
    "            if X[i] not in bsX:\n",
    "                oob_xs.append(X[i])\n",
    "                oob_ys.append(y[i])\n",
    "        #self.oob_data.append((oob_xs, oob_ys))\n",
    "        self.oob_Xs.append(oob_xs)\n",
    "        self.oob_ys.append(oob_ys)\n",
    "        return bsX, bsY\n",
    "\n",
    "    # return list of bootstrapped datasets (1 for each tree in forest)\n",
    "    def get_datasets(self, X, y):\n",
    "        bootstrap_sets = []\n",
    "        for i in range(self.num_trees): # for each tree in the forest\n",
    "            bootstrap_sets.append(self.bootstrap(X, y)) # get boostrapped sample, add to list\n",
    "        return bootstrap_sets\n",
    "\n",
    "    # grow forest\n",
    "    def grow_forest(self, X, y):\n",
    "        datasets = self.get_datasets(X, y) # get bootstrapped datasets to use building trees\n",
    "        #self.datasets = self.get_datasets(X, y) # get bootstrapped datasets to use building trees (maybe don't need instance var for this)\n",
    "        for i in range(self.num_trees): # for each tree and each dataset\n",
    "            tree = DecisionTree(self.max_depth)\n",
    "            #tree.build_tree(X, y)\n",
    "            tree.build_tree(datasets[i][0], datasets[i][1]) # build tree with next bootstrapped dataset\n",
    "            #tree.build_tree(self.datasets[i][0], self.datasets[i][1]) # build tree with next bootstrapped dataset\n",
    "            # is this where i need to get oob predictions?\n",
    "            self.forest.append(tree) # add tree to forest\n",
    "    \n",
    "    # evaluate accuracy rate of random forest classifier\n",
    "    def calc_oob_accuracy(self):\n",
    "        tot_accuracy = 0\n",
    "        for i in range(self.num_trees): # for each tree in the forest\n",
    "            preds = []\n",
    "            for x in self.oob_Xs[i]: # for each row in that tree's oob sample\n",
    "                preds.append(self.forest[i].predict(x)) # use that tree to get a prediction for that row\n",
    "            tot_accuracy += calc_accuracy(preds, self.oob_ys[i]) # keep running total of accuracy\n",
    "        return tot_accuracy / self.num_trees # return average accuracy across all trees\n",
    "    \n",
    "    # predict classification for new data\n",
    "    def predict(self, x):\n",
    "        preds = []\n",
    "        # for each tree in this forest\n",
    "        for tree in self.forest:\n",
    "            # predict this data point with that tree\n",
    "            # add the prediction to the list\n",
    "            preds.append(tree.predict(x))\n",
    "        # return the most common prediction from all trees\n",
    "        try:\n",
    "            return mode(preds)\n",
    "        # if there's a tie, randomly select one class to return\n",
    "        except:\n",
    "            print(\"tie\")\n",
    "            return choice(list(set(preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Random Forest\n",
    "\n",
    "In the following code blocks, we will create a new instance of the RandomForest class, grow the forest using training data, and calculate the out of bag accuracy. I also include timing functions to get a sense of how long this code takes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest was grown in  106.4427  seconds.\n",
      "OOB accuracy was calculated in  0.8918  seconds.\n",
      "OOB accuracy:  74.66 %\n"
     ]
    }
   ],
   "source": [
    "# build forest\n",
    "myForest = RandomForest(num_trees=100, depth=10)\n",
    "start_forest = time.perf_counter()\n",
    "myForest.grow_forest(X_train, y_train)\n",
    "finish_forest = time.perf_counter()\n",
    "forest_time = np.round(finish_forest - start_forest, 4)\n",
    "\n",
    "# assess accuracy\n",
    "start_oob = time.perf_counter()\n",
    "oob_accy = myForest.calc_oob_accuracy()\n",
    "finish_oob = time.perf_counter()\n",
    "oob_time = np.round(finish_oob - start_oob, 4)\n",
    "\n",
    "# print results\n",
    "print(\"Forest was grown in \", forest_time, \" seconds.\")\n",
    "print(\"OOB accuracy was calculated in \", oob_time, \" seconds.\")\n",
    "print(\"OOB accuracy: \", np.round(oob_accy * 100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"evaluation.png\">\n",
    "\n",
    "#### Evaluating the Forest\n",
    "\n",
    "Finally, let's test this random forest's accuracy by using it to classify new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest testing accuracy:  77.7 %\n"
     ]
    }
   ],
   "source": [
    "# test forest on new data\n",
    "test_preds = []\n",
    "for x in X_test:\n",
    "    test_preds.append(myForest.predict(x))\n",
    "\n",
    "forest_accuracy = np.round(calc_accuracy(test_preds, y_test), 4)\n",
    "print(\"random forest testing accuracy: \", forest_accuracy*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Steps\n",
    "In time there are changes and additions I would like to make to this code so it can do more with a greater variety of data. Here are some examples:\n",
    "\n",
    "* modify code to accept categorical (vs. only numerical) predicting variables\n",
    "* use similar code to build regression tree from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "https://www.kaggle.com/gauravdesurkar/lt-vehicle-loan-default-prediction\n",
    "\n",
    "http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15381-s06/www/DTs.pdf\n",
    "\n",
    "https://towardsdatascience.com/what-is-out-of-bag-oob-score-in-random-forest-a7fa23d710\n",
    "\n",
    "https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8\n",
    "\n",
    "https://medium.com/analytics-steps/understanding-the-gini-index-and-information-gain-in-decision-trees-ab4720518ba8\n",
    "\n",
    "https://towardsdatascience.com/gini-index-vs-information-entropy-7a7e4fed3fcb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
